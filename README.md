# Human-Emotion-and-Gesture-Detector
Understanding how to build a human emotion and gesture detector with Deep Learning from scratch.

Check out these articles for a complete understanding of the project and the code:

https://towardsdatascience.com/human-emotion-and-gesture-detector-using-deep-learning-part-1-d0023008d0eb
https://towardsdatascience.com/human-emotion-and-gesture-detector-using-deep-learning-part-2-471724f7a023

1. Data pre-processing consists of all the steps for extraction of images from the fer2013.csv file which can be obtained from Kaggle.
2. EDA consists of the complete Exploratory Data Analysis for both the emotions and gestures model. 
3. Emotions_train is the first emotions model we have built with the help of data Augmentation.
4. gestures_train is the gestures model we have built using the VGG-16 transfer learning model with additional custom layers.
5. emotions_final consists of the second emotions model with better accuracy.
6. Recordings.py consists of the complete code for all the recordings I have used for both emotions and gestures.
7. final_run.py gives the user an option to pick between emotions or gestures detection.
9. final_run1.py runs both emotions and gestures detection simultaneously.
